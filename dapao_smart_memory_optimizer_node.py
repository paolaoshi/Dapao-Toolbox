import gc

import psutil
import torch
from comfy.comfy_types import IO


class DapaoSmartMemoryOptimizerNode:
    DESCRIPTION = "ç”¨é€”ï¼šæ˜¾å­˜/å†…å­˜æ™ºèƒ½ä¼˜åŒ–ï¼ˆé¢„ç•™æ˜¾å­˜ã€ä½å†…å­˜/ä½æ˜¾å­˜æ—¶å¸è½½æ¨¡å‹ã€æ¸…ç©ºç¼“å­˜ã€GCï¼‰ã€‚\næ”¾ç½®ä½ç½®ï¼šå»ºè®®æ”¾åœ¨å·¥ä½œæµæœ€å‰é¢ï¼›æˆ–åœ¨åŠ è½½å¤§æ¨¡å‹/é«˜åˆ†è¾¨ç‡é‡‡æ ·å‰å†æ”¾ä¸€ä¸ªã€‚\nç›´é€šç”¨æ³•ï¼šæŠŠâ€œğŸ”Œ ä»»æ„è¾“å…¥â€æ¥åœ¨ä½ æƒ³è§¦å‘çš„ä½ç½®ï¼Œè¾“å‡ºç«¯ç»§ç»­æ¥å›åŸæµç¨‹ï¼Œå³å¯ç²¾ç¡®æ§åˆ¶æœ¬èŠ‚ç‚¹ä½•æ—¶æ‰§è¡Œã€‚\nä¿¡æ¯å±•ç¤ºï¼šè¿è¡Œåä¿¡æ¯ä¼šç›´æ¥æ˜¾ç¤ºåœ¨èŠ‚ç‚¹é‡Œï¼ˆæ— éœ€æ¥ä¿¡æ¯è¾“å‡ºç«¯å£ï¼‰ã€‚\nå‚æ•°è¯´æ˜ï¼š\n- é¢„ç•™æ˜¾å­˜GBï¼šå†™å…¥ ComfyUI çš„é¢„ç•™æ˜¾å­˜è®¾ç½®ï¼Œç»™ç³»ç»Ÿ/å…¶å®ƒç¨‹åºç•™æ˜¾å­˜ï¼›è¶Šå¤§è¶Šç¨³ï¼Œä½†å¯ç”¨æ˜¾å­˜è¶Šå°‘ã€‚\n- å†…å­˜å®‰å…¨ä½™é‡GBï¼šRAM å¯ç”¨ä½äºæ­¤å€¼æ—¶è§¦å‘â€œå¸è½½å…¨éƒ¨æ¨¡å‹ + æ¸…ç†â€ã€‚\n- æ˜¾å­˜å®‰å…¨ä½™é‡GBï¼šVRAM å¯ç”¨ä½äºæ­¤å€¼æ—¶è§¦å‘â€œå¸è½½éƒ¨åˆ†æ¨¡å‹å ç”¨ + æ¸…ç†â€ã€‚\n- ä½å†…å­˜æ—¶å¸è½½å…¨éƒ¨æ¨¡å‹ï¼šæ›´æ¿€è¿›ï¼Œé€‚åˆçˆ†å†…å­˜/é¢‘ç¹åˆ‡å›¾åœºæ™¯ã€‚\n- è¿è¡Œæ—¶æ¸…ç©ºç¼“å­˜ï¼šæ¯æ¬¡è¿è¡Œéƒ½åšä¸€æ¬¡ç¼“å­˜æ¸…ç†ï¼Œèƒ½ç¼“è§£ç¢ç‰‡ä½†å¯èƒ½ç•¥æ…¢ã€‚\n- å¼ºåˆ¶GCï¼šå¼ºåˆ¶ Python åƒåœ¾å›æ”¶ï¼Œé…åˆæ¸…ç†æ›´å½»åº•ä½†å¯èƒ½å¡ä¸€ä¸‹ã€‚\nå¦‚ä½•åˆ¤æ–­ç”Ÿæ•ˆï¼šçœ‹èŠ‚ç‚¹å†…æ˜¾ç¤ºçš„â€œåŠ¨ä½œ=â€¦â€å’Œâ€œé¢„ç•™æ˜¾å­˜=â€¦â€ï¼Œä»¥åŠè¿è¡Œå‰å RAM/VRAM å¯ç”¨æ•°å€¼å˜åŒ–ã€‚"

    @classmethod
    def IS_CHANGED(cls, **kwargs):
        return float("NaN")

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "âœ… å¯ç”¨": ("BOOLEAN", {"default": True, "tooltip": "æ€»å¼€å…³ï¼›å…³é—­æ—¶ä¸åšä»»ä½•ä¼˜åŒ–ï¼Œä»…è¾“å‡ºçŠ¶æ€"}),
                "ğŸ™ é¢„ç•™æ˜¾å­˜GB": ("FLOAT", {"default": 0.6, "min": 0.0, "max": 256.0, "step": 0.1, "tooltip": "é¢„ç•™ç»™ç³»ç»Ÿ/å…¶å®ƒç¨‹åºçš„æ˜¾å­˜(GB)ã€‚è¶Šå¤§è¶Šç¨³ï¼Œä½†å¯ç”¨æ˜¾å­˜è¶Šå°‘"}),
                "ğŸ§  å†…å­˜å®‰å…¨ä½™é‡GB": ("FLOAT", {"default": 4.0, "min": 0.0, "max": 512.0, "step": 0.5, "tooltip": "RAMå¯ç”¨ä½äºæ­¤å€¼(GB)æ—¶è§¦å‘â€œå¸è½½å…¨éƒ¨æ¨¡å‹+æ¸…ç†â€"}),
                "ğŸ§  æ˜¾å­˜å®‰å…¨ä½™é‡GB": ("FLOAT", {"default": 0.0, "min": 0.0, "max": 256.0, "step": 0.1, "tooltip": "VRAMå¯ç”¨ä½äºæ­¤å€¼(GB)æ—¶è§¦å‘â€œå¸è½½éƒ¨åˆ†æ¨¡å‹å ç”¨+æ¸…ç†â€ï¼›0è¡¨ç¤ºä¸å¯ç”¨è¯¥è§„åˆ™"}),
                "ğŸ§¹ ä½å†…å­˜æ—¶å¸è½½å…¨éƒ¨æ¨¡å‹": ("BOOLEAN", {"default": True, "tooltip": "å½“RAMä¸è¶³æ—¶å¸è½½æ‰€æœ‰å·²åŠ è½½æ¨¡å‹ï¼Œå›æ”¶æ›´å½»åº•ä½†ä¼šè§¦å‘åç»­é‡æ–°åŠ è½½"}),
                "ğŸ§½ è¿è¡Œæ—¶æ¸…ç©ºç¼“å­˜": ("BOOLEAN", {"default": True, "tooltip": "æ¯æ¬¡è¿è¡Œéƒ½æ¸…ç†ä¸€æ¬¡ç¼“å­˜ï¼Œç¼“è§£ç¢ç‰‡ï¼Œä½†å¯èƒ½ç•¥æ…¢"}),
                "ğŸ§¯ å¼ºåˆ¶GC": ("BOOLEAN", {"default": True, "tooltip": "å¼ºåˆ¶Pythonåƒåœ¾å›æ”¶(gc.collect)ï¼Œå¯èƒ½æ›´å½»åº•ä½†ä¼šæœ‰çŸ­æš‚åœé¡¿"}),
            },
            "optional": {
                "ğŸ”Œ ä»»æ„è¾“å…¥": (IO.ANY, {"tooltip": "ä»»æ„ç±»å‹ç›´é€šè¾“å…¥ï¼šæŠŠå®ƒæ¥åœ¨ä½ æƒ³è§¦å‘ä¼˜åŒ–çš„ç¯èŠ‚ä¸­é—´"}),
            },
            "hidden": {
                "unique_id": "UNIQUE_ID",
            },
        }

    RETURN_TYPES = (IO.ANY,)
    RETURN_NAMES = ("ğŸ”Œ ä»»æ„è¾“å‡º",)
    FUNCTION = "optimize"
    CATEGORY = "ğŸ¤–Dapao-Toolbox/ğŸ§ æ€§èƒ½ä¼˜åŒ–"
    OUTPUT_NODE = True

    def _format_bytes(self, n: int) -> str:
        if n is None:
            return "0"
        if n >= 1024**3:
            return f"{n / (1024**3):.2f}GB"
        return f"{n / (1024**2):.0f}MB"

    def optimize(self, **kwargs):
        import comfy.model_management as model_management

        any_in = kwargs.get("ğŸ”Œ ä»»æ„è¾“å…¥", None)
        unique_id = kwargs.get("unique_id", kwargs.get("UNIQUE_ID", None))

        enabled = bool(kwargs.get("âœ… å¯ç”¨", True))
        reserve_vram_gb = float(kwargs.get("ğŸ™ é¢„ç•™æ˜¾å­˜GB", 0.6))
        min_ram_gb = float(kwargs.get("ğŸ§  å†…å­˜å®‰å…¨ä½™é‡GB", 4.0))
        min_vram_gb = float(kwargs.get("ğŸ§  æ˜¾å­˜å®‰å…¨ä½™é‡GB", 0.0))
        unload_on_low_ram = bool(kwargs.get("ğŸ§¹ ä½å†…å­˜æ—¶å¸è½½å…¨éƒ¨æ¨¡å‹", True))
        clear_cache = bool(kwargs.get("ğŸ§½ è¿è¡Œæ—¶æ¸…ç©ºç¼“å­˜", True))
        force_gc = bool(kwargs.get("ğŸ§¯ å¼ºåˆ¶GC", True))

        dev = model_management.get_torch_device()
        vm = psutil.virtual_memory()
        ram_total = int(vm.total)
        ram_avail = int(vm.available)

        old_reserved = int(getattr(model_management, "EXTRA_RESERVED_VRAM", 0))

        actions = []
        if not enabled:
            info = (
                f"çŠ¶æ€=å…³é—­ | è®¾å¤‡={dev} | "
                f"RAMå¯ç”¨={self._format_bytes(ram_avail)}/{self._format_bytes(ram_total)} | "
                f"é¢„ç•™æ˜¾å­˜={self._format_bytes(old_reserved)}"
            )
            if unique_id is not None:
                from server import PromptServer
                PromptServer.instance.send_sync("dapao.memopt.info", {"node_id": int(unique_id), "info": info})
            return {"ui": {"dapao_info": info, "text": [info]}, "result": (any_in,)}

        model_management.EXTRA_RESERVED_VRAM = max(0, int(reserve_vram_gb * (1024**3)))
        if model_management.EXTRA_RESERVED_VRAM != old_reserved:
            actions.append(f"é¢„ç•™æ˜¾å­˜:{self._format_bytes(old_reserved)}â†’{self._format_bytes(model_management.EXTRA_RESERVED_VRAM)}")

        if clear_cache:
            model_management.soft_empty_cache()
            actions.append("æ¸…ç©ºç¼“å­˜")

        if unload_on_low_ram and min_ram_gb > 0:
            threshold_ram = int(min_ram_gb * (1024**3))
            if ram_avail < threshold_ram:
                model_management.unload_all_models()
                model_management.free_memory(1e30, torch.device("cpu"))
                if force_gc:
                    gc.collect()
                model_management.soft_empty_cache()
                actions.append("ä½å†…å­˜å¸è½½æ¨¡å‹")

        vram_total = 0
        vram_free = 0
        if hasattr(dev, "type") and dev.type not in ("cpu", "mps"):
            try:
                vram_total = int(model_management.get_total_memory(dev))
                # è·å–æ›´è¯¦ç»†çš„å†…å­˜ä¿¡æ¯ï¼šTotal_Free = OS_Free + Torch_Reserved_Free
                mem_free_total, mem_free_torch = model_management.get_free_memory(dev, torch_free_too=True)
                # è®¡ç®—çœŸå®çš„ OS å±‚é¢å‰©ä½™æ˜¾å­˜ï¼ˆTask Manager çœ‹åˆ°çš„ç©ºé—²ï¼‰
                os_free = mem_free_total - mem_free_torch
            except Exception:
                vram_total = 0
                mem_free_total = 0
                os_free = 0

            reserved_bytes = int(getattr(model_management, "EXTRA_RESERVED_VRAM", 0))
            
            # ç­–ç•¥å‡çº§ï¼šåŒé‡æ£€æŸ¥
            # 1. æ£€æŸ¥ Torch è‡ªèº«å ç”¨æ˜¯å¦è¶Šç•Œ (Self-Discipline)
            #    å¦‚æœ (Torchå·²å ç”¨ + é¢„ç•™) > æ€»æ˜¾å­˜ï¼Œè¯´æ˜ ComfyUI å ç”¨äº†æœ¬è¯¥é¢„ç•™çš„ç©ºé—´ï¼Œå¿…é¡»åå‡ºæ¥
            torch_reserved_bytes = 0
            try:
                torch_reserved_bytes = torch.cuda.memory_reserved(dev)
            except:
                pass
            
            # 2. æ£€æŸ¥ç³»ç»Ÿå±‚é¢æ˜¯å¦çœŸçš„æ‹¥æŒ¤ (System Pressure)
            #    è™½ç„¶ os_free æœ‰æ—¶è™šé«˜ï¼Œä½†å¦‚æœå®ƒçœŸçš„å¾ˆä½ï¼Œé‚£è‚¯å®šè¦å¸è½½
            
            should_unload_reserved = False
            
            # åˆ¤å®š A: Torch è‡ªèº«å ç”¨è¿‡é«˜
            if reserved_bytes > 0 and vram_total > 0:
                max_allowed_torch = vram_total - reserved_bytes
                if torch_reserved_bytes > max_allowed_torch:
                    should_unload_reserved = True
            
            # åˆ¤å®š B: ç³»ç»Ÿå‰©ä½™æ˜¾å­˜ä¸è¶³ (è¾…åŠ©åˆ¤æ–­)
            if reserved_bytes > 0 and os_free < reserved_bytes:
                should_unload_reserved = True
                
            if should_unload_reserved:
                # 1. å…ˆå°è¯•è½»é‡çº§æ¸…ç©º
                torch.cuda.empty_cache()
                torch.cuda.ipc_collect()
                
                # é‡æ–°æ£€æŸ¥
                try:
                    torch_reserved_bytes = torch.cuda.memory_reserved(dev)
                    mem_free_total, mem_free_torch = model_management.get_free_memory(dev, torch_free_too=True)
                    os_free = mem_free_total - mem_free_torch
                except:
                    pass
                    
                # å†æ¬¡ç¡®è®¤æ˜¯å¦è¿˜éœ€è¦å¸è½½æ¨¡å‹
                still_need_unload = False
                if vram_total > 0 and torch_reserved_bytes > (vram_total - reserved_bytes):
                    still_need_unload = True
                if os_free < reserved_bytes:
                    still_need_unload = True
                    
                if still_need_unload:
                    # è®¡ç®—éœ€è¦é‡Šæ”¾å¤šå°‘ï¼š
                    # ç›®æ ‡æ˜¯è®© torch_reserved <= (vram_total - reserved_bytes)
                    # æˆ–è€… os_free >= reserved_bytes
                    # è¿™é‡Œç›´æ¥ç»™ä¸€ä¸ªè¶³å¤Ÿå¤§çš„å€¼è®© ComfyUI å°½åŠ›é‡Šæ”¾ï¼Œæˆ–è€…æŒ‰å·®å€¼é‡Šæ”¾
                    # ä¸ºäº†ä¿é™©ï¼Œç›´æ¥å°è¯•é‡Šæ”¾é¢„ç•™çš„å¤§å°ï¼Œæˆ–è€…æ›´å¤š
                    
                    target_free = reserved_bytes
                    # å¦‚æœæ˜¯å› ä¸º Torch å å¤ªå¤šï¼Œç®—å‡ºè¶…é¢éƒ¨åˆ†
                    if vram_total > 0:
                        excess = torch_reserved_bytes - (vram_total - reserved_bytes)
                        if excess > 0:
                            target_free = max(target_free, excess)
                    
                    model_management.free_memory(target_free, dev)
                    torch.cuda.empty_cache()
                    torch.cuda.ipc_collect()
                    actions.append("æŒ‰é¢„ç•™å¸è½½æ¨¡å‹")
                    
                    # æ›´æ–°çŠ¶æ€
                    try:
                        mem_free_total, mem_free_torch = model_management.get_free_memory(dev, torch_free_too=True)
                        torch_reserved_bytes = torch.cuda.memory_reserved(dev)
                    except:
                        pass

            if min_vram_gb > 0:
                need_free = int(min_vram_gb * (1024**3))
                # é€»è¾‘å¯ç”¨æ˜¾å­˜ = æ€»æ˜¾å­˜ - Torchå ç”¨ - é¢„ç•™
                # æˆ–è€… = mem_free_total - reserved_bytes (ComfyUIè§†è§’)
                # ä¸ºäº†æ›´å®‰å…¨ï¼Œæˆ‘ä»¬ç”¨æ›´ä¿å®ˆçš„è®¡ç®—ï¼š
                # å‡è®¾ Torch è¿˜èƒ½ç”³è¯·åˆ°çš„æœ€å¤§å€¼æ˜¯ (vram_total - reserved_bytes - torch_reserved_bytes)
                # ä½† mem_free_total å·²ç»åŒ…å«äº† torch_reserved é‡Œæœªä½¿ç”¨çš„éƒ¨åˆ†
                # æ‰€ä»¥è¿˜æ˜¯ç”¨ mem_free_total - reserved_bytes æ¯”è¾ƒç¬¦åˆ ComfyUI å®šä¹‰
                
                vram_free_effective = max(0, mem_free_total - reserved_bytes)
                if vram_free_effective < need_free:
                    # éœ€è¦è…¾å‡º (need_free + reserved_bytes) æ‰èƒ½ä¿è¯å®‰å…¨
                    model_management.free_memory(need_free + reserved_bytes, dev)
                    torch.cuda.empty_cache()
                    actions.append("ä½æ˜¾å­˜å¸è½½æ¨¡å‹")

        if force_gc and not unload_on_low_ram:
            gc.collect()
            actions.append("GC")

        vm2 = psutil.virtual_memory()
        ram_avail2 = int(vm2.available)
        if hasattr(dev, "type") and dev.type not in ("cpu", "mps"):
            try:
                reserved_bytes2 = int(getattr(model_management, "EXTRA_RESERVED_VRAM", 0))
                mem_free_total2, mem_free_torch2 = model_management.get_free_memory(dev, torch_free_too=True)
                os_free2 = mem_free_total2 - mem_free_torch2
                torch_reserved2 = torch.cuda.memory_reserved(dev)
                
                # VRAMå¯ç”¨(é€»è¾‘)ï¼šComfyUI è¿˜èƒ½ç”¨å¤šå°‘ (æ‰£é™¤é¢„ç•™)
                vram_free_logical = max(0, mem_free_total2 - reserved_bytes2)
                
                # Torchå ç”¨ï¼šå½“å‰ Torch å®é™…ç”³è¯·äº†å¤šå°‘
                torch_usage_str = self._format_bytes(torch_reserved2)
                
            except Exception:
                mem_free_total2 = 0
                os_free2 = 0
                vram_free_logical = 0
                torch_reserved2 = 0
                torch_usage_str = "0"
        else:
            mem_free_total2 = 0
            os_free2 = 0
            vram_free_logical = 0
            torch_reserved2 = 0
            torch_usage_str = "0"
  
        action_text = "ï¼›".join(actions) if actions else "æ— "
        info = (
            f"åŠ¨ä½œ={action_text} | è®¾å¤‡={dev} | "
            f"RAMå¯ç”¨={self._format_bytes(ram_avail2)}/{self._format_bytes(ram_total)} | "
            f"VRAMå¯ç”¨(é€»è¾‘)={self._format_bytes(vram_free_logical)} | "
            f"ç³»ç»Ÿå‰©ä½™={self._format_bytes(os_free2)} | "
            f"Torchå ç”¨={torch_usage_str} | "
            f"é¢„ç•™è®¾ç½®={self._format_bytes(int(model_management.EXTRA_RESERVED_VRAM))}"
        )
        if unique_id is not None:
            from server import PromptServer
            PromptServer.instance.send_sync("dapao.memopt.info", {"node_id": int(unique_id), "info": info})
        return {"ui": {"dapao_info": info, "text": [info]}, "result": (any_in,)}


NODE_CLASS_MAPPINGS = {
    "DapaoSmartMemoryOptimizerNode": DapaoSmartMemoryOptimizerNode,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "DapaoSmartMemoryOptimizerNode": "ğŸ™æ˜¾å­˜ä¸¨å†…å­˜æ™ºèƒ½ä¼˜åŒ–@ç‚®è€å¸ˆçš„å°è¯¾å ‚",
}
