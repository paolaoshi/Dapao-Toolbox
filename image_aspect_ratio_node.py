import torch
import numpy as np
from PIL import Image, ImageOps

class ImageAspectRatioResizeNode:
    """
    æŒ‰å®½é«˜æ¯”ç¼©æ”¾èŠ‚ç‚¹
    
    åŠŸèƒ½è¯´æ˜ï¼š
    - æ”¯æŒå¤šç§é¢„è®¾å®½é«˜æ¯”å’Œè‡ªå®šä¹‰æ¯”ä¾‹
    - æ”¯æŒæŒ‰è¾¹é•¿ã€æœ€é•¿è¾¹ã€æœ€çŸ­è¾¹ç¼©æ”¾
    - æ”¯æŒé€‚åº”(Letterbox)ã€è£å‰ª(Crop)ã€æ‹‰ä¼¸(Stretch)æ¨¡å¼
    - æ”¯æŒé®ç½©(Mask)åŒæ­¥å¤„ç†
    """
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "ğŸ“¸ å›¾åƒ": ("IMAGE",),
                "ğŸ“ å®½é«˜æ¯”": (["åŸå›¾", "è‡ªå®šä¹‰", "1:1", "16:9", "4:3", "3:2", "2:3", "9:16", "3:4", "21:9", "9:21"], {
                    "default": "åŸå›¾",
                    "tooltip": "é€‰æ‹©ç›®æ ‡å®½é«˜æ¯”ï¼ŒåŸå›¾=ä¿æŒåŸå§‹æ¯”ä¾‹ï¼Œè‡ªå®šä¹‰=æ‰‹åŠ¨è®¾ç½®æ¯”ä¾‹"
                }),
                "ğŸ“ æ¯”ä¾‹å®½åº¦": ("INT", {
                    "default": 1,
                    "min": 1,
                    "max": 10000,
                    "step": 1,
                    "tooltip": "è‡ªå®šä¹‰å®½é«˜æ¯”çš„å®½åº¦å€¼ï¼ˆå½“å®½é«˜æ¯”é€‰'è‡ªå®šä¹‰'æ—¶ç”Ÿæ•ˆï¼‰"
                }),
                "ğŸ“ æ¯”ä¾‹é«˜åº¦": ("INT", {
                    "default": 1,
                    "min": 1,
                    "max": 10000,
                    "step": 1,
                    "tooltip": "è‡ªå®šä¹‰å®½é«˜æ¯”çš„é«˜åº¦å€¼ï¼ˆå½“å®½é«˜æ¯”é€‰'è‡ªå®šä¹‰'æ—¶ç”Ÿæ•ˆï¼‰"
                }),
                "ğŸ¨ é€‚åº”æ¨¡å¼": (["åŒ…å«", "è£å‰ª", "æ‹‰ä¼¸"], {
                    "default": "åŒ…å«",
                    "tooltip": "åŒ…å«=é»‘è¾¹å¡«å……(Letterbox), è£å‰ª=å……æ»¡ç”»é¢(Crop), æ‹‰ä¼¸=å˜å½¢å……æ»¡(Stretch)"
                }),
                "ğŸ” ç¼©æ”¾ç®—æ³•": (["lanczos", "bicubic", "bilinear", "nearest"], {
                    "default": "lanczos",
                    "tooltip": "å›¾åƒç¼©æ”¾æ’å€¼ç®—æ³•"
                }),
                "ğŸ”¢ å°ºå¯¸å€æ•°": ("INT", {
                    "default": 8,
                    "min": 1,
                    "max": 64,
                    "step": 1,
                    "tooltip": "ç¡®ä¿è¾“å‡ºå°ºå¯¸æ˜¯è¯¥æ•°å€¼çš„å€æ•°ï¼ˆé€šå¸¸ä¸º8ï¼‰"
                }),
                "ğŸ“ é”å®šè¾¹é•¿": (["ä¸é”å®š", "é”å®šå®½åº¦", "é”å®šé«˜åº¦", "é”å®šæœ€é•¿è¾¹", "é”å®šæœ€çŸ­è¾¹"], {
                    "default": "ä¸é”å®š",
                    "tooltip": "é€‰æ‹©è¦é”å®šçš„è¾¹é•¿åŸºå‡†"
                }),
                "ğŸ“ é”å®šé•¿åº¦": ("INT", {
                    "default": 1024,
                    "min": 1,
                    "max": 16384,
                    "step": 1,
                    "tooltip": "é”å®šè¾¹çš„ç›®æ ‡é•¿åº¦åƒç´ å€¼"
                }),
                "ğŸŒˆ èƒŒæ™¯é¢œè‰²": ("STRING", {
                    "default": "#000000",
                    "multiline": False,
                    "tooltip": "åŒ…å«(Letterbox)æ¨¡å¼ä¸‹çš„å¡«å……èƒŒæ™¯è‰²(Hexæ ¼å¼)"
                })
            },
            "optional": {
                "ğŸ˜· é®ç½©": ("MASK",)
            }
        }

    RETURN_TYPES = ("IMAGE", "MASK", "INT", "INT", "INT")
    RETURN_NAMES = ("ğŸ–¼ï¸ å›¾åƒ", "ğŸ˜· é®ç½©", "ğŸ“ åŸå§‹å°ºå¯¸", "ğŸ“ å®½åº¦", "ğŸ“ é«˜åº¦")
    FUNCTION = "resize_image"
    CATEGORY = "ğŸ¤–Dapao-Toolbox"

    def resize_image(self, **kwargs):
        # è·å–å‚æ•° (ä½¿ç”¨ä¸­æ–‡é”®å)
        image = kwargs.get("ğŸ“¸ å›¾åƒ")
        aspect_ratio = kwargs.get("ğŸ“ å®½é«˜æ¯”", "åŸå›¾")
        proportional_width = kwargs.get("ğŸ“ æ¯”ä¾‹å®½åº¦", 1)
        proportional_height = kwargs.get("ğŸ“ æ¯”ä¾‹é«˜åº¦", 1)
        fit_mode = kwargs.get("ğŸ¨ é€‚åº”æ¨¡å¼", "åŒ…å«")
        method = kwargs.get("ğŸ” ç¼©æ”¾ç®—æ³•", "lanczos")
        round_to_multiple = kwargs.get("ğŸ”¢ å°ºå¯¸å€æ•°", 8)
        scale_to_side = kwargs.get("ğŸ“ é”å®šè¾¹é•¿", "ä¸é”å®š")
        scale_to_length = kwargs.get("ğŸ“ é”å®šé•¿åº¦", 1024)
        background_color = kwargs.get("ğŸŒˆ èƒŒæ™¯é¢œè‰²", "#000000")
        mask = kwargs.get("ğŸ˜· é®ç½©", None)
        
        # å‚æ•°æ˜ å°„
        fit_mode_map = {
            "åŒ…å«": "letterbox",
            "è£å‰ª": "crop",
            "æ‹‰ä¼¸": "stretch"
        }
        fit_mode_en = fit_mode_map.get(fit_mode, "letterbox")
        
        scale_to_side_map = {
            "ä¸é”å®š": "None",
            "é”å®šå®½åº¦": "Width",
            "é”å®šé«˜åº¦": "Height",
            "é”å®šæœ€é•¿è¾¹": "Longest",
            "é”å®šæœ€çŸ­è¾¹": "Shortest"
        }
        scale_to_side_en = scale_to_side_map.get(scale_to_side, "None")
        
        # è½¬æ¢æ–¹æ³•
        method_map = {
            "lanczos": Image.Resampling.LANCZOS,
            "bicubic": Image.Resampling.BICUBIC,
            "bilinear": Image.Resampling.BILINEAR,
            "nearest": Image.Resampling.NEAREST
        }
        resample_method = method_map.get(method, Image.Resampling.LANCZOS)
        
        # å¤„ç† batch
        result_images = []
        result_masks = []
        
        # ç¡®ä¿ image æ˜¯ list (batch)
        if len(image.shape) < 4:
            image = image.unsqueeze(0)
            
        batch_size = image.shape[0]
        
        # å¤„ç† mask
        if mask is not None:
            if len(mask.shape) < 3:
                mask = mask.unsqueeze(0)
            # å¦‚æœ mask batch å°äº image batchï¼Œéœ€è¦å¹¿æ’­
            if mask.shape[0] < batch_size:
                mask = mask.repeat(batch_size, 1, 1)
        
        original_width = 0
        original_height = 0
        final_width = 0
        final_height = 0

        for i in range(batch_size):
            # 1. è½¬æ¢ä¸º PIL
            img_tensor = image[i]
            img_pil = self.tensor2pil(img_tensor)
            
            w, h = img_pil.size
            original_width = w
            original_height = h
            
            # 2. è®¡ç®—ç›®æ ‡å®½é«˜æ¯”
            target_ratio = w / h
            if aspect_ratio != "åŸå›¾":
                if aspect_ratio == "è‡ªå®šä¹‰":
                    target_ratio = proportional_width / proportional_height
                else:
                    try:
                        w_ratio, h_ratio = map(float, aspect_ratio.split(":"))
                        target_ratio = w_ratio / h_ratio
                    except:
                        target_ratio = w / h

            # 3. è®¡ç®—ç›®æ ‡å°ºå¯¸
            target_w = w
            target_h = h
            
            if scale_to_side_en == "None":
                # ä¸å¼ºåˆ¶æŒ‡å®šè¾¹é•¿ï¼Œæ ¹æ® fit_mode å’Œå®½é«˜æ¯”è®¡ç®—
                if fit_mode_en == "letterbox" or fit_mode_en == "stretch":
                    # åŒ…å«åŸå›¾ï¼šå¦‚æœåŸå›¾è¾ƒå®½ï¼Œä»¥å®½ä¸ºå‡†ï¼›å¦‚æœåŸå›¾è¾ƒé«˜ï¼Œä»¥é«˜ä¸ºå‡†
                    # ä½†è¿™é‡Œæˆ‘ä»¬éœ€è¦å¾—åˆ°ç›®æ ‡å®½é«˜æ¯”
                    # Letterbox: ç›®æ ‡æ¡†åŒ…å«åŸå›¾ã€‚
                    # å¦‚æœ w/h > target_ratio (åŸå›¾æ›´å®½)ï¼Œåˆ™å®½ä¸å˜ï¼Œé«˜å¢åŠ  -> target_w = w, target_h = w / target_ratio
                    # å¦‚æœ w/h < target_ratio (åŸå›¾æ›´é«˜)ï¼Œåˆ™é«˜ä¸å˜ï¼Œå®½å¢åŠ  -> target_h = h, target_w = h * target_ratio
                    if w / h > target_ratio:
                        target_w = w
                        target_h = int(w / target_ratio)
                    else:
                        target_h = h
                        target_w = int(h * target_ratio)
                elif fit_mode_en == "crop":
                    # è£å‰ªåŸå›¾ï¼šç›®æ ‡æ¡†åœ¨åŸå›¾å†…
                    # å¦‚æœ w/h > target_ratio (åŸå›¾æ›´å®½)ï¼Œåˆ™é«˜ä¸å˜ï¼Œå®½å‡å° -> target_h = h, target_w = h * target_ratio
                    # å¦‚æœ w/h < target_ratio (åŸå›¾æ›´é«˜)ï¼Œåˆ™å®½ä¸å˜ï¼Œé«˜å‡å° -> target_w = w, target_h = w / target_ratio
                    if w / h > target_ratio:
                        target_h = h
                        target_w = int(h * target_ratio)
                    else:
                        target_w = w
                        target_h = int(w / target_ratio)
            else:
                # æŒ‡å®šäº†åŸºå‡†è¾¹å’Œé•¿åº¦
                length = scale_to_length
                if scale_to_side_en == "Width":
                    target_w = length
                    target_h = int(length / target_ratio)
                elif scale_to_side_en == "Height":
                    target_h = length
                    target_w = int(length * target_ratio)
                elif scale_to_side_en == "Longest":
                    if target_ratio >= 1: # å®½ >= é«˜
                        target_w = length
                        target_h = int(length / target_ratio)
                    else:
                        target_h = length
                        target_w = int(length * target_ratio)
                elif scale_to_side_en == "Shortest":
                    if target_ratio >= 1: # å®½ >= é«˜ï¼Œé«˜æ˜¯çŸ­è¾¹
                        target_h = length
                        target_w = int(length * target_ratio)
                    else: # å®½æ˜¯çŸ­è¾¹
                        target_w = length
                        target_h = int(length / target_ratio)
            
            # 4. å››èˆäº”å…¥å¯¹é½
            if round_to_multiple > 1:
                target_w = (target_w + round_to_multiple - 1) // round_to_multiple * round_to_multiple
                target_h = (target_h + round_to_multiple - 1) // round_to_multiple * round_to_multiple
            
            final_width = target_w
            final_height = target_h
            
            # 5. æ‰§è¡Œç¼©æ”¾å¤„ç†
            
            # å‡†å¤‡èƒŒæ™¯é¢œè‰²
            bg_color_rgb = self.hex_to_rgb(background_color)
            
            # åˆ›å»ºç›®æ ‡ç”»å¸ƒ
            new_img = Image.new("RGB", (target_w, target_h), bg_color_rgb)
            
            # å¯¹åº”çš„ mask ç”»å¸ƒ (é»‘è‰²èƒŒæ™¯)
            new_mask = Image.new("L", (target_w, target_h), 0)
            
            # è·å–å½“å‰ mask (å¦‚æœæœ‰)
            current_mask = None
            if mask is not None:
                current_mask = self.tensor2pil_mask(mask[i])
            
            if fit_mode_en == "stretch":
                # æ‹‰ä¼¸æ¨¡å¼ï¼šç›´æ¥ç¼©æ”¾åˆ°ç›®æ ‡å°ºå¯¸
                resized_img = img_pil.resize((target_w, target_h), resample_method)
                new_img.paste(resized_img, (0, 0))
                
                if current_mask:
                    resized_mask = current_mask.resize((target_w, target_h), resample_method)
                    new_mask.paste(resized_mask, (0, 0))
                else:
                    # å¦‚æœæ²¡æœ‰è¾“å…¥ maskï¼Œæ‹‰ä¼¸æ¨¡å¼ä¸‹é»˜è®¤å…¨ç™½ mask (è¡¨ç¤ºå…¨å›¾æœ‰æ•ˆ)
                    new_mask = Image.new("L", (target_w, target_h), 255)
                    
            elif fit_mode_en == "crop":
                # è£å‰ªæ¨¡å¼ï¼šå…ˆä¿æŒæ¯”ä¾‹ç¼©æ”¾åˆ°è¦†ç›–ç›®æ ‡å°ºå¯¸ï¼Œç„¶åå±…ä¸­è£å‰ª
                # è®¡ç®—ç¼©æ”¾æ¯”ä¾‹ï¼šå–å®½æ¯”å’Œé«˜æ¯”ä¸­è¾ƒå¤§çš„é‚£ä¸ªï¼ˆä¿è¯è¦†ç›–ï¼‰
                scale = max(target_w / w, target_h / h)
                scaled_w = int(w * scale)
                scaled_h = int(h * scale)
                
                resized_img = img_pil.resize((scaled_w, scaled_h), resample_method)
                
                # è®¡ç®—å±…ä¸­è£å‰ªä½ç½®
                left = (scaled_w - target_w) // 2
                top = (scaled_h - target_h) // 2
                
                # Crop
                cropped_img = resized_img.crop((left, top, left + target_w, top + target_h))
                new_img.paste(cropped_img, (0, 0))
                
                if current_mask:
                    resized_mask = current_mask.resize((scaled_w, scaled_h), resample_method)
                    cropped_mask = resized_mask.crop((left, top, left + target_w, top + target_h))
                    new_mask.paste(cropped_mask, (0, 0))
                else:
                    new_mask = Image.new("L", (target_w, target_h), 255)

            else: # letterbox (é»˜è®¤)
                # é€‚åº”æ¨¡å¼ï¼šä¿æŒæ¯”ä¾‹ç¼©æ”¾åˆ°åŒ…å«åœ¨ç›®æ ‡å°ºå¯¸å†…ï¼Œå±…ä¸­ï¼Œå¡«å……èƒŒæ™¯
                # è®¡ç®—ç¼©æ”¾æ¯”ä¾‹ï¼šå–å®½æ¯”å’Œé«˜æ¯”ä¸­è¾ƒå°çš„é‚£ä¸ªï¼ˆä¿è¯åŒ…å«ï¼‰
                scale = min(target_w / w, target_h / h)
                scaled_w = int(w * scale)
                scaled_h = int(h * scale)
                
                resized_img = img_pil.resize((scaled_w, scaled_h), resample_method)
                
                # è®¡ç®—å±…ä¸­ä½ç½®
                left = (target_w - scaled_w) // 2
                top = (target_h - scaled_h) // 2
                
                new_img.paste(resized_img, (left, top))
                
                if current_mask:
                    resized_mask = current_mask.resize((scaled_w, scaled_h), resample_method)
                    new_mask.paste(resized_mask, (left, top))
                else:
                    # åŸå›¾åŒºåŸŸä¸ºç™½ï¼ŒèƒŒæ™¯ä¸ºé»‘
                    white_block = Image.new("L", (scaled_w, scaled_h), 255)
                    new_mask.paste(white_block, (left, top))
            
            result_images.append(self.pil2tensor(new_img))
            result_masks.append(self.pil2tensor_mask(new_mask))

        # åˆå¹¶ batch
        final_images_tensor = torch.cat(result_images, dim=0)
        final_masks_tensor = torch.cat(result_masks, dim=0)
        
        # è¿”å› 5 ä¸ªå€¼ï¼Œå¯¹åº” 5 ä¸ª RETURN_TYPES
        return (final_images_tensor, final_masks_tensor, original_width, final_width, final_height)

    def hex_to_rgb(self, hex_color):
        """å°†åå…­è¿›åˆ¶é¢œè‰²è½¬æ¢ä¸ºRGBå…ƒç»„"""
        hex_color = hex_color.lstrip('#')
        try:
            return tuple(int(hex_color[i:i+2], 16) for i in (0, 2, 4))
        except:
            return (0, 0, 0)

    def tensor2pil(self, image):
        return Image.fromarray(np.clip(255. * image.cpu().numpy(), 0, 255).astype(np.uint8))

    def pil2tensor(self, image):
        return torch.from_numpy(np.array(image).astype(np.float32) / 255.0).unsqueeze(0)
        
    def tensor2pil_mask(self, mask):
        return Image.fromarray(np.clip(255. * mask.cpu().numpy(), 0, 255).astype(np.uint8), mode='L')

    def pil2tensor_mask(self, mask):
        return torch.from_numpy(np.array(mask).astype(np.float32) / 255.0).unsqueeze(0)
