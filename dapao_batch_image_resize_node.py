import torch
import numpy as np
import os
from PIL import Image, ImageOps

class DapaoBatchImageResize:
    def __init__(self):
        pass

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "ğŸ“Š ç¼©æ”¾æ¨¡å¼": (["ğŸ“ æŒ‰é•¿è¾¹ç¼©æ”¾", "ğŸ“ æŒ‰çŸ­è¾¹ç¼©æ”¾", "ğŸ”¢ å¼ºåˆ¶æ‹‰ä¼¸è‡³æŒ‡å®šå°ºå¯¸", "âœ‚ï¸ ç¼©æ”¾å¹¶è£å‰ªè‡³æŒ‡å®šå°ºå¯¸"], {"default": "âœ‚ï¸ ç¼©æ”¾å¹¶è£å‰ªè‡³æŒ‡å®šå°ºå¯¸"}),
                "ğŸ”¢ å°ºå¯¸åŸºå‡†": ("INT", {"default": 1024, "min": 64, "max": 8192, "step": 8, "tooltip": "ä»…åœ¨æŒ‰é•¿/çŸ­è¾¹ç¼©æ”¾æ¨¡å¼ä¸‹æœ‰æ•ˆ"}),
                "â†”ï¸ ç›®æ ‡å®½åº¦": ("INT", {"default": 512, "min": 64, "max": 8192, "step": 8}),
                "â†•ï¸ ç›®æ ‡é«˜åº¦": ("INT", {"default": 512, "min": 64, "max": 8192, "step": 8}),
                "ğŸ“ è£å‰ªä½ç½®": (["å±…ä¸­", "é¡¶éƒ¨å±…ä¸­", "åº•éƒ¨å±…ä¸­", "å·¦ä¾§å±…ä¸­", "å³ä¾§å±…ä¸­", "å·¦ä¸Š", "å³ä¸Š", "å·¦ä¸‹", "å³ä¸‹"], {"default": "å±…ä¸­"}),
                "ğŸ”¨ é‡‡æ ·ç®—æ³•": (["nearest", "bilinear", "bicubic", "lanczos"], {"default": "lanczos"}),
            },
            "optional": {
                "ğŸ–¼ï¸ å›¾åƒè¾“å…¥": ("IMAGE",),
                "ğŸ“‚ æœ¬åœ°æ–‡ä»¶å¤¹è·¯å¾„": ("STRING", {"default": "", "multiline": False}),
            }
        }

    RETURN_TYPES = ("IMAGE",)
    RETURN_NAMES = ("ğŸ–¼ï¸ å¤„ç†åå›¾åƒ",)
    FUNCTION = "batch_resize"
    CATEGORY = "ğŸ¤–Dapao-Toolbox"
    INPUT_IS_LIST = True

    def batch_resize(self, **kwargs):
        # æå–å‚æ•° (ç”±äº INPUT_IS_LIST=True, æ‰€æœ‰è¾“å…¥éƒ½æ˜¯åˆ—è¡¨ï¼Œéœ€è¦è§£åŒ…)
        mode = kwargs.get("ğŸ“Š ç¼©æ”¾æ¨¡å¼", ["âœ‚ï¸ ç¼©æ”¾å¹¶è£å‰ªè‡³æŒ‡å®šå°ºå¯¸"])[0]
        size_value = kwargs.get("ğŸ”¢ å°ºå¯¸åŸºå‡†", [1024])[0]
        target_w = kwargs.get("â†”ï¸ ç›®æ ‡å®½åº¦", [512])[0]
        target_h = kwargs.get("â†•ï¸ ç›®æ ‡é«˜åº¦", [512])[0]
        crop_pos = kwargs.get("ğŸ“ è£å‰ªä½ç½®", ["å±…ä¸­"])[0]
        algo_str = kwargs.get("ğŸ”¨ é‡‡æ ·ç®—æ³•", ["lanczos"])[0]
        
        images_input = kwargs.get("ğŸ–¼ï¸ å›¾åƒè¾“å…¥", None)
        folder_path_list = kwargs.get("ğŸ“‚ æœ¬åœ°æ–‡ä»¶å¤¹è·¯å¾„", [""])
        folder_path = folder_path_list[0] if folder_path_list else ""

        # æ˜ å°„é‡‡æ ·ç®—æ³•
        algo_map = {
            "nearest": Image.NEAREST,
            "bilinear": Image.BILINEAR,
            "bicubic": Image.BICUBIC,
            "lanczos": Image.LANCZOS
        }
        resample_algo = algo_map.get(algo_str, Image.LANCZOS)

        pil_images = []

        # 1. å¤„ç†å›¾åƒè¾“å…¥
        if images_input is not None:
            # images_input æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œé‡Œé¢å¯èƒ½åŒ…å«å¤šä¸ª Tensor [B, H, W, C]
            for img_batch in images_input:
                if isinstance(img_batch, torch.Tensor):
                    # [B, H, W, C] -> split to single images
                    for i in range(img_batch.shape[0]):
                        pil_img = self.tensor_to_pil(img_batch[i])
                        pil_images.append(pil_img)

        # 2. å¤„ç†æ–‡ä»¶å¤¹è¾“å…¥
        if folder_path and os.path.isdir(folder_path):
            valid_exts = {'.jpg', '.jpeg', '.png', '.webp', '.bmp', '.tiff'}
            try:
                for root, _, files in os.walk(folder_path):
                    for file in files:
                        if os.path.splitext(file)[1].lower() in valid_exts:
                            try:
                                img_path = os.path.join(root, file)
                                pil_img = Image.open(img_path)
                                # ç»Ÿä¸€è½¬ä¸º RGBA æˆ– RGBï¼Œé¿å…åç»­å¤„ç†å‡ºé”™
                                if pil_img.mode not in ["RGB", "RGBA"]:
                                    pil_img = pil_img.convert("RGBA")
                                pil_images.append(pil_img)
                            except Exception as e:
                                print(f"DapaoBatchImageResize: Failed to load {file}: {e}")
            except Exception as e:
                print(f"DapaoBatchImageResize: Error reading folder {folder_path}: {e}")

        if not pil_images:
            # å¦‚æœæ²¡æœ‰å›¾ç‰‡ï¼Œè¿”å›ä¸€ä¸ªç©ºçš„ Tensor (1, 1, 1, 3) é¿å…æŠ¥é”™ï¼Œæˆ–è€…ç›´æ¥æŠ¥é”™
            # è¿™é‡Œé€‰æ‹©è¿”å›ç©ºåˆ—è¡¨ï¼Œä½† ComfyUI ä¸‹æ¸¸å¯èƒ½ä¼šæŠ¥é”™
            print("DapaoBatchImageResize: No images found.")
            return ([],)

        processed_images = []

        for img in pil_images:
            w, h = img.size
            new_img = None

            if mode == "ğŸ“ æŒ‰é•¿è¾¹ç¼©æ”¾":
                scale = size_value / max(w, h)
                new_w = int(w * scale)
                new_h = int(h * scale)
                new_img = img.resize((new_w, new_h), resample=resample_algo)

            elif mode == "ğŸ“ æŒ‰çŸ­è¾¹ç¼©æ”¾":
                scale = size_value / min(w, h)
                new_w = int(w * scale)
                new_h = int(h * scale)
                new_img = img.resize((new_w, new_h), resample=resample_algo)

            elif mode == "ğŸ”¢ å¼ºåˆ¶æ‹‰ä¼¸è‡³æŒ‡å®šå°ºå¯¸":
                new_img = img.resize((target_w, target_h), resample=resample_algo)

            elif mode == "âœ‚ï¸ ç¼©æ”¾å¹¶è£å‰ªè‡³æŒ‡å®šå°ºå¯¸":
                # æ ¸å¿ƒé€»è¾‘ï¼šå…ˆç¼©æ”¾ï¼ˆè¦†ç›–ï¼‰ï¼Œå†è£å‰ª
                # è®¡ç®—è¦†ç›–æ‰€éœ€çš„æ¯”ä¾‹
                scale_w = target_w / w
                scale_h = target_h / h
                scale = max(scale_w, scale_h) # å–æœ€å¤§å€¼ä»¥ç¡®ä¿è¦†ç›–
                
                resize_w = int(w * scale)
                resize_h = int(h * scale)
                
                # ä¸ºäº†ç²¾åº¦ï¼Œå‘ä¸Šå–æ•´æˆ–å¤šåŠ ä¸€ç‚¹ç‚¹é˜²æ­¢é»‘è¾¹ï¼Ÿintè½¬æ¢é€šå¸¸å‘ä¸‹å–æ•´ã€‚
                # å¦‚æœ resize_w < target_w (ç”±äºç²¾åº¦ä¸¢å¤±)ï¼Œä¼šæœ‰é»‘è¾¹ã€‚
                # å»ºè®®ä½¿ç”¨ math.ceil æˆ–è€… +0.5
                if resize_w < target_w: resize_w = target_w
                if resize_h < target_h: resize_h = target_h

                img_resized = img.resize((resize_w, resize_h), resample=resample_algo)
                
                # è£å‰ªé€»è¾‘
                left, top = 0, 0
                if crop_pos == "å±…ä¸­":
                    left = (resize_w - target_w) // 2
                    top = (resize_h - target_h) // 2
                elif crop_pos == "é¡¶éƒ¨å±…ä¸­":
                    left = (resize_w - target_w) // 2
                    top = 0
                elif crop_pos == "åº•éƒ¨å±…ä¸­":
                    left = (resize_w - target_w) // 2
                    top = resize_h - target_h
                elif crop_pos == "å·¦ä¾§å±…ä¸­":
                    left = 0
                    top = (resize_h - target_h) // 2
                elif crop_pos == "å³ä¾§å±…ä¸­":
                    left = resize_w - target_w
                    top = (resize_h - target_h) // 2
                elif crop_pos == "å·¦ä¸Š":
                    left = 0
                    top = 0
                elif crop_pos == "å³ä¸Š":
                    left = resize_w - target_w
                    top = 0
                elif crop_pos == "å·¦ä¸‹":
                    left = 0
                    top = resize_h - target_h
                elif crop_pos == "å³ä¸‹":
                    left = resize_w - target_w
                    top = resize_h - target_h
                
                right = left + target_w
                bottom = top + target_h
                
                new_img = img_resized.crop((left, top, right, bottom))

            if new_img:
                processed_images.append(self.pil_to_tensor(new_img))

        # å°è¯•å †å  Tensor
        # åªæœ‰å½“æ‰€æœ‰å›¾åƒå°ºå¯¸ä¸€è‡´æ—¶æ‰èƒ½ stack
        if not processed_images:
            return ([],)

        first_shape = processed_images[0].shape
        can_stack = True
        for p_img in processed_images:
            if p_img.shape != first_shape:
                can_stack = False
                break
        
        if can_stack:
            # stack [1, H, W, C] -> [B, H, W, C]
            output_tensor = torch.cat(processed_images, dim=0)
            return (output_tensor,)
        else:
            # è¿”å›åˆ—è¡¨ï¼ŒComfyUI åº”è¯¥èƒ½å¤„ç† list of tensors (å¦‚æœä¸ stack)
            # ä½†æ˜¯æ ‡å‡†çš„ ComfyUI èŠ‚ç‚¹ä¸‹æ¸¸é€šå¸¸æœŸæœ› Tensor Batchã€‚
            # å¦‚æœä¸èƒ½ stackï¼Œç›´æ¥è¿”å› listã€‚
            # ä¸‹æ¸¸èŠ‚ç‚¹å¦‚æœä¸å¼€å¯ INPUT_IS_LIST å¯èƒ½ä¼šåªå¤„ç†ç¬¬ä¸€ä¸ªæˆ–è€…æŠ¥é”™ã€‚
            # ä½†ä½œä¸º ToolBoxï¼Œå°½é‡å…¼å®¹ã€‚
            return (processed_images,)

    def tensor_to_pil(self, tensor):
        # tensor: [H, W, C]
        return Image.fromarray(np.clip(255. * tensor.cpu().numpy(), 0, 255).astype(np.uint8))

    def pil_to_tensor(self, pil_image):
        # return: [1, H, W, C]
        return torch.from_numpy(np.array(pil_image).astype(np.float32) / 255.0).unsqueeze(0)
