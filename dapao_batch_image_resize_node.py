import torch
import numpy as np
import os
import io
from PIL import Image, ImageOps

class DapaoBatchImageResize:
    def __init__(self):
        pass

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "ğŸ“Š ç¼©æ”¾æ¨¡å¼": (["ğŸ“ æŒ‰é•¿è¾¹ç¼©æ”¾", "ğŸ“ æŒ‰çŸ­è¾¹ç¼©æ”¾", "ğŸ”¢ å¼ºåˆ¶æ‹‰ä¼¸è‡³æŒ‡å®šå°ºå¯¸", "âœ‚ï¸ ç¼©æ”¾å¹¶è£å‰ªè‡³æŒ‡å®šå°ºå¯¸"], {"default": "âœ‚ï¸ ç¼©æ”¾å¹¶è£å‰ªè‡³æŒ‡å®šå°ºå¯¸"}),
                "ğŸ”¢ ç¼©æ”¾åŸºå‡†": ("INT", {"default": 1024, "min": 64, "max": 8192, "step": 8, "tooltip": "âš ï¸æ³¨æ„ï¼šä»…åœ¨[æŒ‰é•¿è¾¹/çŸ­è¾¹ç¼©æ”¾]æ¨¡å¼ä¸‹ç”Ÿæ•ˆï¼å†³å®šç¼©æ”¾åçš„åŸºå‡†å°ºå¯¸ã€‚"}),
                "â†”ï¸ è£å‰ªå®½åº¦": ("INT", {"default": 512, "min": 64, "max": 8192, "step": 8, "tooltip": "âš ï¸æ³¨æ„ï¼šä»…åœ¨[å¼ºåˆ¶æ‹‰ä¼¸]å’Œ[ç¼©æ”¾å¹¶è£å‰ª]æ¨¡å¼ä¸‹ç”Ÿæ•ˆï¼"}),
                "â†•ï¸ è£å‰ªé«˜åº¦": ("INT", {"default": 512, "min": 64, "max": 8192, "step": 8, "tooltip": "âš ï¸æ³¨æ„ï¼šä»…åœ¨[å¼ºåˆ¶æ‹‰ä¼¸]å’Œ[ç¼©æ”¾å¹¶è£å‰ª]æ¨¡å¼ä¸‹ç”Ÿæ•ˆï¼"}),
                "ğŸ“ è£å‰ªä½ç½®": (["å±…ä¸­", "é¡¶éƒ¨å±…ä¸­", "åº•éƒ¨å±…ä¸­", "å·¦ä¾§å±…ä¸­", "å³ä¾§å±…ä¸­", "å·¦ä¸Š", "å³ä¸Š", "å·¦ä¸‹", "å³ä¸‹"], {"default": "å±…ä¸­"}),
                "ğŸ”¨ é‡‡æ ·ç®—æ³•": (["nearest", "bilinear", "bicubic", "lanczos"], {"default": "lanczos"}),
                "ğŸ’¾ ä¿å­˜æ¨¡å¼": (["âŒ ä¸ä¿å­˜ (ä»…é¢„è§ˆ)", "âš ï¸ è¦†ç›–åŸæ–‡ä»¶", "ğŸ“ ä¿å­˜åˆ°æ–°æ–‡ä»¶å¤¹"], {"default": "âŒ ä¸ä¿å­˜ (ä»…é¢„è§ˆ)"}),
                "ğŸ“‚ è¾“å‡ºæ–‡ä»¶å¤¹å": ("STRING", {"default": "resized_output", "multiline": False, "tooltip": "ä»…åœ¨'ä¿å­˜åˆ°æ–°æ–‡ä»¶å¤¹'æ¨¡å¼ä¸‹æœ‰æ•ˆï¼Œå°†åœ¨åŸå›¾ç‰‡ç›®å½•ä¸‹åˆ›å»ºæ­¤æ–‡ä»¶å¤¹"}),
                "ğŸ’¾ é™åˆ¶æ–‡ä»¶å¤§å° (MB)": ("FLOAT", {"default": 0, "min": 0, "max": 100, "step": 0.1, "tooltip": "0è¡¨ç¤ºä¸é™åˆ¶ã€‚ä»…å¯¹æ”¯æŒå‹ç¼©çš„æ ¼å¼(å¦‚JPG/WEBP)æœ‰æ•ˆ"}),
                "ğŸ“‰ ä¿å­˜è´¨é‡": ("INT", {"default": 95, "min": 1, "max": 100, "step": 1, "tooltip": "ä¿å­˜å›¾ç‰‡çš„è´¨é‡ (1-100)"}),
            },
            "optional": {
                "ğŸ–¼ï¸ å›¾åƒè¾“å…¥": ("IMAGE",),
                "ğŸ“‚ æœ¬åœ°æ–‡ä»¶å¤¹è·¯å¾„": ("STRING", {"default": "", "multiline": False}),
            }
        }

    RETURN_TYPES = ("IMAGE",)
    RETURN_NAMES = ("ğŸ–¼ï¸ å¤„ç†åå›¾åƒ",)
    FUNCTION = "batch_resize"
    CATEGORY = "ğŸ¤–Dapao-Toolbox"
    INPUT_IS_LIST = True
    OUTPUT_NODE = True

    def batch_resize(self, **kwargs):
        # æå–å‚æ•°
        mode = kwargs.get("ğŸ“Š ç¼©æ”¾æ¨¡å¼", ["âœ‚ï¸ ç¼©æ”¾å¹¶è£å‰ªè‡³æŒ‡å®šå°ºå¯¸"])[0]
        size_value = kwargs.get("ğŸ”¢ ç¼©æ”¾åŸºå‡†", [1024])[0]
        target_w = kwargs.get("â†”ï¸ è£å‰ªå®½åº¦", [512])[0]
        target_h = kwargs.get("â†•ï¸ è£å‰ªé«˜åº¦", [512])[0]
        crop_pos = kwargs.get("ğŸ“ è£å‰ªä½ç½®", ["å±…ä¸­"])[0]
        algo_str = kwargs.get("ğŸ”¨ é‡‡æ ·ç®—æ³•", ["lanczos"])[0]
        
        save_mode = kwargs.get("ğŸ’¾ ä¿å­˜æ¨¡å¼", ["âŒ ä¸ä¿å­˜ (ä»…é¢„è§ˆ)"])[0]
        output_folder_name = kwargs.get("ğŸ“‚ è¾“å‡ºæ–‡ä»¶å¤¹å", ["resized_output"])[0]
        max_file_size_mb = kwargs.get("ğŸ’¾ é™åˆ¶æ–‡ä»¶å¤§å° (MB)", [0])[0]
        save_quality = kwargs.get("ğŸ“‰ ä¿å­˜è´¨é‡", [95])[0]

        images_input = kwargs.get("ğŸ–¼ï¸ å›¾åƒè¾“å…¥", None)
        folder_path_list = kwargs.get("ğŸ“‚ æœ¬åœ°æ–‡ä»¶å¤¹è·¯å¾„", [""])
        folder_path = folder_path_list[0] if folder_path_list else ""

        # æ˜ å°„é‡‡æ ·ç®—æ³•
        algo_map = {
            "nearest": Image.NEAREST,
            "bilinear": Image.BILINEAR,
            "bicubic": Image.BICUBIC,
            "lanczos": Image.LANCZOS
        }
        resample_algo = algo_map.get(algo_str, Image.LANCZOS)

        # ç»´æŠ¤ä¸€ä¸ª (pil_img, original_path) çš„åˆ—è¡¨
        # original_path ä¸º None è¡¨ç¤ºæ¥è‡ª Tensor è¾“å…¥ï¼Œæ— æ³•è¦†ç›–ä¿å­˜
        image_data_list = []

        # 1. å¤„ç†å›¾åƒè¾“å…¥ (Tensor)
        if images_input is not None:
            for img_batch in images_input:
                if isinstance(img_batch, torch.Tensor):
                    for i in range(img_batch.shape[0]):
                        pil_img = self.tensor_to_pil(img_batch[i])
                        image_data_list.append((pil_img, None))

        # 2. å¤„ç†æ–‡ä»¶å¤¹è¾“å…¥
        if folder_path and os.path.isdir(folder_path):
            valid_exts = {'.jpg', '.jpeg', '.png', '.webp', '.bmp', '.tiff'}
            try:
                for root, _, files in os.walk(folder_path):
                    for file in files:
                        ext = os.path.splitext(file)[1].lower()
                        if ext in valid_exts:
                            try:
                                img_path = os.path.join(root, file)
                                pil_img = Image.open(img_path)
                                # ç»Ÿä¸€è½¬ä¸º RGBA æˆ– RGB
                                if pil_img.mode not in ["RGB", "RGBA"]:
                                    pil_img = pil_img.convert("RGBA")
                                image_data_list.append((pil_img, img_path))
                            except Exception as e:
                                print(f"DapaoBatchImageResize: Failed to load {file}: {e}")
            except Exception as e:
                print(f"DapaoBatchImageResize: Error reading folder {folder_path}: {e}")

        if not image_data_list:
            print("DapaoBatchImageResize: No images found.")
            return ([],)

        processed_images = []

        for pil_img, original_path in image_data_list:
            w, h = pil_img.size
            new_img = None

            # --- ç¼©æ”¾/è£å‰ªé€»è¾‘ ---
            if mode == "ğŸ“ æŒ‰é•¿è¾¹ç¼©æ”¾":
                scale = size_value / max(w, h)
                new_w = int(w * scale)
                new_h = int(h * scale)
                new_img = pil_img.resize((new_w, new_h), resample=resample_algo)

            elif mode == "ğŸ“ æŒ‰çŸ­è¾¹ç¼©æ”¾":
                scale = size_value / min(w, h)
                new_w = int(w * scale)
                new_h = int(h * scale)
                new_img = pil_img.resize((new_w, new_h), resample=resample_algo)

            elif mode == "ğŸ”¢ å¼ºåˆ¶æ‹‰ä¼¸è‡³æŒ‡å®šå°ºå¯¸":
                new_img = pil_img.resize((target_w, target_h), resample=resample_algo)

            elif mode == "âœ‚ï¸ ç¼©æ”¾å¹¶è£å‰ªè‡³æŒ‡å®šå°ºå¯¸":
                scale_w = target_w / w
                scale_h = target_h / h
                scale = max(scale_w, scale_h)
                
                resize_w = int(w * scale)
                resize_h = int(h * scale)
                
                if resize_w < target_w: resize_w = target_w
                if resize_h < target_h: resize_h = target_h

                img_resized = pil_img.resize((resize_w, resize_h), resample=resample_algo)
                
                left, top = 0, 0
                if crop_pos == "å±…ä¸­":
                    left = (resize_w - target_w) // 2
                    top = (resize_h - target_h) // 2
                elif crop_pos == "é¡¶éƒ¨å±…ä¸­":
                    left = (resize_w - target_w) // 2
                    top = 0
                elif crop_pos == "åº•éƒ¨å±…ä¸­":
                    left = (resize_w - target_w) // 2
                    top = resize_h - target_h
                elif crop_pos == "å·¦ä¾§å±…ä¸­":
                    left = 0
                    top = (resize_h - target_h) // 2
                elif crop_pos == "å³ä¾§å±…ä¸­":
                    left = resize_w - target_w
                    top = (resize_h - target_h) // 2
                elif crop_pos == "å·¦ä¸Š":
                    left = 0
                    top = 0
                elif crop_pos == "å³ä¸Š":
                    left = resize_w - target_w
                    top = 0
                elif crop_pos == "å·¦ä¸‹":
                    left = 0
                    top = resize_h - target_h
                elif crop_pos == "å³ä¸‹":
                    left = resize_w - target_w
                    top = resize_h - target_h
                
                right = left + target_w
                bottom = top + target_h
                
                new_img = img_resized.crop((left, top, right, bottom))

            if new_img:
                # æ·»åŠ åˆ°è¾“å‡ºåˆ—è¡¨
                processed_images.append(self.pil_to_tensor(new_img))
                
                # --- ä¿å­˜é€»è¾‘ ---
                if save_mode != "âŒ ä¸ä¿å­˜ (ä»…é¢„è§ˆ)" and original_path:
                    try:
                        save_path = ""
                        # ç¡®å®šä¿å­˜è·¯å¾„
                        if save_mode == "âš ï¸ è¦†ç›–åŸæ–‡ä»¶":
                            save_path = original_path
                        elif save_mode == "ğŸ“ ä¿å­˜åˆ°æ–°æ–‡ä»¶å¤¹":
                            dir_name = os.path.dirname(original_path)
                            file_name = os.path.basename(original_path)
                            new_dir = os.path.join(dir_name, output_folder_name)
                            if not os.path.exists(new_dir):
                                os.makedirs(new_dir)
                            save_path = os.path.join(new_dir, file_name)
                        
                        # ç¡®å®šä¿å­˜æ ¼å¼
                        ext = os.path.splitext(save_path)[1].lower()
                        format_map = {
                            '.jpg': 'JPEG', '.jpeg': 'JPEG',
                            '.png': 'PNG', '.webp': 'WEBP',
                            '.bmp': 'BMP', '.tiff': 'TIFF'
                        }
                        # å¦‚æœæ²¡æœ‰æ‰©å±•åæˆ–è€…ä¸è¯†åˆ«ï¼Œé»˜è®¤ç”¨ PNG (å¦‚æœæ˜¯å¦å­˜ä¸ºï¼Œåº”è¯¥æœ‰æ‰©å±•åï¼›å¦‚æœæ˜¯è¦†ç›–ï¼Œè‚¯å®šæœ‰)
                        save_format = format_map.get(ext, 'PNG')

                        # å¤„ç† RGBA -> RGB (å¦‚æœä¿å­˜ä¸º JPEG)
                        img_to_save = new_img
                        if save_format == 'JPEG' and img_to_save.mode == 'RGBA':
                            img_to_save = img_to_save.convert('RGB')

                        # --- æ–‡ä»¶å¤§å°é™åˆ¶é€»è¾‘ ---
                        current_quality = save_quality
                        
                        # åªæœ‰æ”¯æŒè´¨é‡å‚æ•°çš„æ ¼å¼æ‰è¿›è¡Œå¾ªç¯å‹ç¼©
                        if max_file_size_mb > 0 and save_format in ['JPEG', 'WEBP']:
                            target_size_bytes = max_file_size_mb * 1024 * 1024
                            min_quality = 10
                            
                            # äºŒåˆ†æ³•æŸ¥æ‰¾åˆé€‚çš„ quality
                            # å®é™…ä¸Šç®€å•çš„å¾ªç¯é€’å‡å¯èƒ½æ›´ç¨³å¥ï¼Œæˆ–è€…å¤šæ¬¡å°è¯•
                            # è¿™é‡Œé‡‡ç”¨ç®€å•çš„å°è¯•ï¼šå¦‚æœå¤§äº†ï¼Œå°±é™è´¨é‡
                            
                            # ç¬¬ä¸€æ¬¡å°è¯•
                            img_byte_arr = io.BytesIO()
                            img_to_save.save(img_byte_arr, format=save_format, quality=current_quality)
                            size = img_byte_arr.tell()
                            
                            if size > target_size_bytes:
                                # å¾ªç¯é™ä½è´¨é‡
                                while size > target_size_bytes and current_quality > min_quality:
                                    current_quality -= 5
                                    img_byte_arr = io.BytesIO()
                                    img_to_save.save(img_byte_arr, format=save_format, quality=current_quality)
                                    size = img_byte_arr.tell()
                                
                            # ä¿å­˜æœ€ç»ˆç»“æœ
                            with open(save_path, "wb") as f:
                                f.write(img_byte_arr.getbuffer())
                                
                        else:
                            # ä¸é™åˆ¶å¤§å°æˆ–ä¸æ”¯æŒå‹ç¼©çš„æ ¼å¼ï¼Œç›´æ¥ä¿å­˜
                            if save_format in ['JPEG', 'WEBP']:
                                img_to_save.save(save_path, quality=current_quality)
                            else:
                                img_to_save.save(save_path)
                                
                        print(f"DapaoBatchImageResize: Saved {save_path}")

                    except Exception as e:
                        print(f"DapaoBatchImageResize: Error saving {original_path}: {e}")

        # å †å  Tensor
        if not processed_images:
            return ([],)

        first_shape = processed_images[0].shape
        can_stack = True
        for p_img in processed_images:
            if p_img.shape != first_shape:
                can_stack = False
                break
        
        if can_stack:
            output_tensor = torch.cat(processed_images, dim=0)
            return (output_tensor,)
        else:
            return (processed_images,)

    def tensor_to_pil(self, tensor):
        return Image.fromarray(np.clip(255. * tensor.cpu().numpy(), 0, 255).astype(np.uint8))

    def pil_to_tensor(self, pil_image):
        return torch.from_numpy(np.array(pil_image).astype(np.float32) / 255.0).unsqueeze(0)
